{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examen Cours Algorithme d'apprentissage par renforcement\n",
    "### Karm Bandit - Recommandation\n",
    "## Algorithme de recommandation Netflix\n",
    "\n",
    "\n",
    "Fabrice Mulotti19/02/2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](static/netflix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Énoncé !\n",
    "NetFlix utilise un certain nombre de variables pour classer les abonnés.   \n",
    "\n",
    "Pour chaque classe d'abonné, Nexflix a déterminé un ensemble **k** de recommandation de film (Candidate Pool) (voir image dessous)   \n",
    "\n",
    "L'algorithme que vous allez concevoir, doit déterminer quel est le film le plus susceptible d'être visionné dans ce pool par l'abonné.   \n",
    "\n",
    "Il sera ainsi placé en haut de la page d'accueil (Daredevil dans notre exemple 2images plus bas)   \n",
    "\n",
    "La récompense est fonction du nombre de fois qu'un nouveau membre de la classe d'abonné visionne effectivement ce film !   \n",
    "\n",
    "Il s'agit d'une ditribution de bernouilli.   \n",
    "\n",
    "`q= np.random.binomial(1,self.p[arm])`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Candidate Pool](static/pool.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sélection](static/exempleSelection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre objectif est de déterminer le choix du film qui apporte la plus grande récompense.   \n",
    "\n",
    "Vous pouvez utiliser l'algorithme de votre choix entre ***epsilon-decay greedy, linUCB ou Gradient.***   \n",
    "\n",
    "Vous aurez sans doute, suivant votre choix, à jouer sur les hyperparamètres.   \n",
    "\n",
    "Les endroits ou vous devrez modifier ou ajouter du code sont répérés par le commentaire :\n",
    "    `# Votre Code`\n",
    "\n",
    "Vous aurez quelques questions à compléter dans la case suivante.\n",
    "\n",
    "## Bon courage !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Vous aurez reconnu un problème du type k arm bandit !\n",
    "\n",
    "### Il y a combien d'actions possibles ?\n",
    "(réponse)\n",
    "\n",
    "### Est ce un problème stationnaire ou non stationnaire ?\n",
    "(réponse)\n",
    "\n",
    "### Une fois votre choix fait, expliquer en quelques mots pourquoi vous avez choisi cet algorithme ?\n",
    "(réponse)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class karm():\n",
    "    __version__ = \"1.3\"\n",
    "    # Votre code : vous aurez sans doute à ajouter des paramètres dans la fonction init selon l'algorithme\n",
    "    def __init__(self,k,seed,initial_value):\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        # nombre de bras\n",
    "        self.k=k\n",
    "    \n",
    "        # initialisation des moyennes des k bras\n",
    "        self.p = np.array([0.25 , 0.30 , 0.35 , 0.10])\n",
    "        \n",
    "        self.initial_value = initial_value\n",
    "        \n",
    "        self.init_tableau()\n",
    "    \n",
    "    def version(self):\n",
    "        #renvoie la version du code\n",
    "        return self.__version__\n",
    "    \n",
    "    def my_argmax(self,my_array):\n",
    "        # notre argmax\n",
    "        my_max=float(\"-inf\")\n",
    "        my_list_of_max = []\n",
    "        for i in range(0,my_array.shape[0]):\n",
    "            if my_array[i] > my_max:\n",
    "                my_max=my_array[i]\n",
    "                my_list_of_max = [ i ]\n",
    "            else:\n",
    "                if my_array[i] == my_max:\n",
    "                    my_list_of_max.append(i)\n",
    "        return(np.random.choice(my_list_of_max))\n",
    "          \n",
    "    def init_tableau(self):\n",
    "        # Initialise les tableaux de suivi\n",
    "        # comptage du nombre de fois qu'un bras est actionné\n",
    "        self.n=np.zeros((self.k))\n",
    "        # init de la récompense par bras\n",
    "        self.Reward=np.full((self.k),self.initial_value)\n",
    "                    \n",
    "    def show(self):\n",
    "        print(\"k   = \",self.k)\n",
    "        print(\"n   = \",self.n)\n",
    "        print(\"R   = \",self.Reward)\n",
    "     \n",
    "    def env(self,arm):\n",
    "        # renvoie la récompense en fonction du bras actionné\n",
    "        q= np.random.binomial(1,self.p[arm])\n",
    "                \n",
    "        Reward_n = self.Reward[arm]\n",
    "        self.n[arm]=self.n[arm]+1\n",
    "\n",
    "        self.Reward[arm] = Reward_n + 1/self.n[arm] * (q - Reward_n)\n",
    "        return(q)\n",
    "    \n",
    "    def action(self,t):\n",
    "        # Politique par défaut gloutonne\n",
    "        # Votre code Choisissez votre algorithme ici \n",
    "        \n",
    "        # Politique Greedy par défault.\n",
    "        arm=self.my_argmax(self.Reward)\n",
    "        \n",
    "        return(arm)\n",
    "    \n",
    "    def simulation(self,ntry,nsample):\n",
    "        result=np.zeros((ntry,nsample))\n",
    "        hist_action=np.zeros((ntry,nsample))\n",
    "        for i in range(ntry):\n",
    "            self.init_tableau()\n",
    "            for j in range(nsample):\n",
    "                action=self.action(j)\n",
    "                q=self.env(action)\n",
    "                result[i,j]=q\n",
    "                hist_action[i,j]=action\n",
    "        rmean = np.mean(result,axis=0)\n",
    "        return(np.sum( hist_action  == 2,axis=0),rmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres fixes !\n",
    "k=4\n",
    "seed=1001\n",
    "Film = ['Strangers Things', 'Narcos', '13 Reasons Why', 'Orange is the new Black']\n",
    "\n",
    "# Vos Paramètres que vous pouvez modifier !\n",
    "initial_value =  0.0\n",
    "\n",
    "# Votre code : vous aurez sans doute à ajouter des paramètres en fonction de votre algo\n",
    "dummy=karm(k,seed, initial_value )\n",
    "dummy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_essai =200\n",
    "nb_episode = 1000\n",
    "actions_correctes , _ = dummy.simulation(nb_essai,nb_episode)\n",
    "actions_correctes = actions_correctes / nb_essai\n",
    "dummy.show()\n",
    "print(\"Si votre algo devez utiliser Greedy, vous allez choisir  \",Film[np.argmax(dummy.Reward)])\n",
    "print(\"Le meilleur choix est \",Film[np.argmax(dummy.p)])\n",
    "if np.argmax(dummy.Reward) == np.argmax(dummy.p):\n",
    "    print(\"Bravo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"% d actions correctes\")\n",
    "\n",
    "plt.plot(actions_correctes,color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laboRL",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
